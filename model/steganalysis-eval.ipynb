{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11295976,"sourceType":"datasetVersion","datasetId":7063408}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Code taken from https://github.com/albblgb/Deep-Steganalysis","metadata":{}},{"cell_type":"markdown","source":"### Importing necessary libraries and modules","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import Tensor\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.nn.parameter import Parameter\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom tqdm import tqdm\nimport logging\nimport numpy as np\nimport math\nfrom torchvision.utils import save_image\nfrom tqdm.contrib import tzip\n\nimport torch\nimport numpy as np\nimport torchvision\nimport os\nimport glob\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T11:34:00.971598Z","iopub.execute_input":"2025-04-06T11:34:00.971960Z","iopub.status.idle":"2025-04-06T11:34:00.985196Z","shell.execute_reply.started":"2025-04-06T11:34:00.971929Z","shell.execute_reply":"2025-04-06T11:34:00.984260Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Dataloader and Dataset","metadata":{}},{"cell_type":"code","source":"# Dataset and DataLoader definitions\nclass dataset_(Dataset):\n    def __init__(self, cover_dir, stego_dir, transform):\n        self.cover_dir = cover_dir\n        self.stego_dir = stego_dir\n        self.transforms = transform\n        self.cover_filenames = list(sorted(os.listdir(cover_dir)))\n        self.stego_filenames = list(sorted(os.listdir(stego_dir)))\n    \n    def __len__(self):\n        return len(self.cover_filenames)\n    \n    def __getitem__(self, index):\n        cover_paths = os.path.join(self.cover_dir, self.cover_filenames[index])\n        stego_paths = os.path.join(self.stego_dir, self.stego_filenames[index])\n        cover_img = Image.open(cover_paths).convert(\"RGB\")\n        stego_img = Image.open(stego_paths).convert(\"RGB\")\n        if self.transforms:\n            cover_img = self.transforms(cover_img)\n            stego_img = self.transforms(stego_img)\n        cover_label = torch.tensor(0, dtype=torch.long)\n        stego_label = torch.tensor(1, dtype=torch.long)\n        sample = {\"cover\": cover_img, \"stego\": stego_img}\n        sample[\"label\"] = [cover_label, stego_label]\n        return sample\n\n# Data transforms\ntransformation = T.Compose([\n    T.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T11:34:02.843319Z","iopub.execute_input":"2025-04-06T11:34:02.843713Z","iopub.status.idle":"2025-04-06T11:34:02.851344Z","shell.execute_reply.started":"2025-04-06T11:34:02.843684Z","shell.execute_reply":"2025-04-06T11:34:02.850307Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Data loaders\ndef get_train_loader(data_dir, batchsize=4, subset_size=None):\n    dataset = dataset_(os.path.join(data_dir, 'cover'), os.path.join(data_dir, 'stego'), transformation)\n    \n    if subset_size is not None and subset_size < len(dataset):\n        # Create a random subset of specified size\n        indices = np.random.choice(len(dataset), subset_size, replace=False)\n        dataset = Subset(dataset, indices)\n    \n    train_loader = DataLoader(\n        dataset,\n        batch_size=batchsize,\n        shuffle=True,\n        pin_memory=True,\n        drop_last=True\n    )\n    return train_loader\n\ndef get_val_loader(data_dir, batchsize=4):\n    val_loader = DataLoader(\n        dataset_(os.path.join(data_dir, 'cover'), os.path.join(data_dir, 'stego'), transformation),\n        batch_size=batchsize,\n        shuffle=True,\n        pin_memory=False,\n        drop_last=False\n    )\n    return val_loader\n\ndef get_test_loader(data_dir, batch_size):\n    test_sets = ImageFolder(root=data_dir, transform=transformation)\n    test_loader = DataLoader(test_sets, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False)\n    return test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:05:31.017204Z","iopub.execute_input":"2025-04-06T12:05:31.017586Z","iopub.status.idle":"2025-04-06T12:05:31.024497Z","shell.execute_reply.started":"2025-04-06T12:05:31.017546Z","shell.execute_reply":"2025-04-06T12:05:31.023497Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### SRNet","metadata":{}},{"cell_type":"code","source":"class ConvBn(nn.Module):\n    \"\"\"Provides utility to create different types of layers.\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        \"\"\"Constructor.\n        Args:\n            in_channels (int): no. of input channels.\n            out_channels (int): no. of output channels.\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n        )\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns Conv2d followed by BatchNorm.\n\n        Returns:\n            Tensor: Output of Conv2D -> BN.\n        \"\"\"\n        return self.batch_norm(self.conv(inp))\n\n\nclass Type1(nn.Module):\n    \"\"\"Creates type 1 layer of SRNet.\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__()\n        self.convbn = ConvBn(in_channels, out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns type 1 layer of SRNet.\n        Args:\n            inp (Tensor): input tensor.\n        Returns:\n            Tensor: Output of type 1 layer.\n        \"\"\"\n        return self.relu(self.convbn(inp))\n\n\nclass Type2(nn.Module):\n    \"\"\"Creates type 2 layer of SRNet.\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__()\n        self.type1 = Type1(in_channels, out_channels)\n        self.convbn = ConvBn(in_channels, out_channels)\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns type 2 layer of SRNet.\n        Args:\n            inp (Tensor): input tensor.\n        Returns:\n            Tensor: Output of type 2 layer.\n        \"\"\"\n        return inp + self.convbn(self.type1(inp))\n\n\nclass Type3(nn.Module):\n    \"\"\"Creates type 3 layer of SRNet.\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=2,\n            padding=0,\n            bias=False,\n        )\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.type1 = Type1(in_channels, out_channels)\n        self.convbn = ConvBn(out_channels, out_channels)\n        self.pool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns type 3 layer of SRNet.\n        Args:\n            inp (Tensor): input tensor.\n\n        Returns:\n            Tensor: Output of type 3 layer.\n        \"\"\"\n        out = self.batch_norm(self.conv1(inp))\n        out1 = self.pool(self.convbn(self.type1(inp)))\n        return out + out1\n\n\nclass Type4(nn.Module):\n    \"\"\"Creates type 4 layer of SRNet.\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__()\n        self.type1 = Type1(in_channels, out_channels)\n        self.convbn = ConvBn(out_channels, out_channels)\n        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns type 4 layer of SRNet.\n        Args:\n            inp (Tensor): input tensor.\n        Returns:\n            Tensor: Output of type 4 layer.\n        \"\"\"\n        return self.gap(self.convbn(self.type1(inp)))\n\n\n\nclass SRNet(nn.Module):\n    \"\"\"This is SRNet model class.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Constructor.\"\"\"\n        super().__init__()\n        self.type1s = nn.Sequential(Type1(3, 64), Type1(64, 16))\n        self.type2s = nn.Sequential(\n            Type2(16, 16),\n            Type2(16, 16),\n            Type2(16, 16),\n            Type2(16, 16),\n            Type2(16, 16),\n        )\n        self.type3s = nn.Sequential(\n            Type3(16, 16),\n            Type3(16, 64),\n            Type3(64, 128),\n            Type3(128, 256),\n        )\n        self.type4 = Type4(256, 512)\n        self.dense = nn.Linear(512, 2)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns logits for input images.\n        Args:\n            inp (Tensor): input image tensor of shape (Batch, stego_img_channel, stego_img_height, stego_img_width)\n        Returns:\n            Tensor: Logits of shape (Batch, 2)\n        \"\"\"\n \n        out = self.type1s(inp)\n        out = self.type2s(out)\n        out = self.type3s(out)\n        out = self.type4(out)\n        out = out.view(out.size(0), -1)\n        out = self.dense(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T11:20:07.205787Z","iopub.execute_input":"2025-04-06T11:20:07.206285Z","iopub.status.idle":"2025-04-06T11:20:07.221481Z","shell.execute_reply.started":"2025-04-06T11:20:07.206257Z","shell.execute_reply":"2025-04-06T11:20:07.220454Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### XuNet","metadata":{}},{"cell_type":"code","source":"class ImageProcessing(nn.Module):\n    \"\"\"Computes convolution with KV filter over the input tensor.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Constructor\"\"\"\n\n        super().__init__()\n        # pylint: disable=E1101\n        self.kv_filter = nn.Parameter(\n            torch.tensor(\n                [\n                    [-1.0, 2.0, -2.0, 2.0, -1.0],\n                    [2.0, -6.0, 8.0, -6.0, 2.0],\n                    [-2.0, 8.0, -12.0, 8.0, -2.0],\n                    [2.0, -6.0, 8.0, -6.0, 2.0],\n                    [-1.0, 2.0, -2.0, 2.0, -1.0],\n                ],\n            ).view(1, 1, 5, 5)\n            / 12.0\n        )  # pylint: enable=E1101\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns tensor convolved with KV filter\"\"\"\n\n        for i in range(3):\n            if i == 0:\n                features = F.conv2d(inp[:, i, :, :].unsqueeze(dim=1), self.kv_filter, stride=1, padding=2)\n            else:\n                features = torch.cat((features, F.conv2d(inp[:, i, :, :].unsqueeze(dim=1), self.kv_filter, stride=1, padding=2)), dim=1)\n            \n        return features\n\n\nclass ConvBlock(nn.Module):\n    \"\"\"This class returns building block for XuNet class.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        activation: str = \"relu\",\n        abs: str = False,\n    ) -> None:\n        super().__init__()\n\n        if kernel_size == 5:\n            self.padding = 2\n        else:\n            self.padding = 0\n\n        if activation == \"tanh\":\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.ReLU()\n\n        self.abs = abs\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=self.padding,\n            bias=False,\n        )\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.pool = nn.AvgPool2d(kernel_size=5, stride=2, padding=2)\n\n    def forward(self, inp: Tensor) -> Tensor:\n        \"\"\"Returns conv->batch_norm.\"\"\"\n        if self.abs:\n            return self.pool(\n                self.activation(self.batch_norm(torch.abs(self.conv(inp))))\n            )\n        return self.pool(self.activation(self.batch_norm(self.conv(inp))))\n\n\nclass XuNet(nn.Module):\n    \"\"\"This class returns XuNet model.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.ImageProcessingLayer = ImageProcessing()\n        self.layer1 = ConvBlock(\n            3, 8, kernel_size=5, activation=\"tanh\", abs=True\n        )\n        self.layer2 = ConvBlock(8, 16, kernel_size=5, activation=\"tanh\")\n        self.layer3 = ConvBlock(16, 32, kernel_size=1)\n        self.layer4 = ConvBlock(32, 64, kernel_size=1)\n        self.layer5 = ConvBlock(64, 128, kernel_size=1)\n        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.fully_connected = nn.Sequential(\n            nn.Linear(in_features=128, out_features=128),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=128, out_features=2),\n        )\n\n    def forward(self, image: Tensor) -> Tensor:\n        \"\"\"Returns logit for the given tensor.\"\"\"\n        with torch.no_grad():\n            out = self.ImageProcessingLayer(image)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.gap(out)\n        out = out.view(out.size(0), -1)\n        out = self.fully_connected(out)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### YeNet","metadata":{}},{"cell_type":"code","source":"SRM_npy = np.load('/kaggle/input/models/SRM_Kernels.npy')\n\nclass SRM_conv2d(nn.Module):\n    def __init__(self, stride=1, padding=0):\n        super(SRM_conv2d, self).__init__()\n        self.in_channels = 3\n        self.out_channels = 30\n        self.kernel_size = (5, 5)\n        if isinstance(stride, int):\n            self.stride = (stride, stride)\n        else:\n            self.stride = stride\n        if isinstance(padding, int):\n            self.padding = (padding, padding)\n        else:\n            self.padding = padding\n        self.dilation = (1,1)\n        self.transpose = False\n        self.output_padding = (0,)\n        self.groups = 1\n        self.weight = Parameter(torch.Tensor(30, self.in_channels, 5, 5), \\\n                                requires_grad=True)\n        self.bias = Parameter(torch.Tensor(30), \\\n                              requires_grad=True)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.weight.data.numpy()[:] = SRM_npy\n        self.bias.data.zero_()\n\n    def forward(self, input):\n        return F.conv2d(input, self.weight, self.bias, \\\n                        self.stride, self.padding, self.dilation, \\\n                        self.groups)\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, \\\n                 stride=1, with_bn=False):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, \\\n                              stride)\n        self.relu = nn.ReLU()\n        self.with_bn = with_bn\n        if with_bn:\n            self.norm = nn.BatchNorm2d(out_channels)\n        else:\n            self.norm = lambda x: x\n        self.reset_parameters()\n\n    def forward(self, x):\n        return self.norm(self.relu(self.conv(x)))\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.conv.weight)\n        self.conv.bias.data.fill_(0.2)\n        if self.with_bn:\n            self.norm.reset_parameters()\n\nclass YeNet(nn.Module):\n    def __init__(self, with_bn=False, threshold=3):\n        super(YeNet, self).__init__()\n        self.with_bn = with_bn\n        self.preprocessing = SRM_conv2d(1, 0)\n        self.TLU = nn.Hardtanh(-threshold, threshold, True)\n        if with_bn:\n            self.norm1 = nn.BatchNorm2d(30)\n        else:\n            self.norm1 = lambda x: x\n        self.block2 = ConvBlock(30, 30, 3, with_bn=self.with_bn)\n        self.block3 = ConvBlock(30, 30, 3, with_bn=self.with_bn)\n        self.block4 = ConvBlock(30, 30, 3, with_bn=self.with_bn)\n        self.pool1 = nn.AvgPool2d(2, 2)\n        self.block5 = ConvBlock(30, 32, 5, with_bn=self.with_bn)\n        self.pool2 = nn.AvgPool2d(3, 2)\n        self.block6 = ConvBlock(32, 32, 5, with_bn=self.with_bn)\n        self.pool3 = nn.AvgPool2d(3, 2)\n        self.block7 = ConvBlock(32, 32, 5, with_bn=self.with_bn)\n        self.pool4 = nn.AvgPool2d(3, 2)\n        self.block8 = ConvBlock(32, 16, 3, with_bn=self.with_bn)\n        self.block9 = ConvBlock(16, 16, 3, 3, with_bn=self.with_bn)\n        self.num_of_neurons = 144 \n       \n        self.ip1 = nn.Linear(self.num_of_neurons, 2)\n        self.reset_parameters()\n\n    def forward(self, x):\n        x = x.float()\n        x = self.preprocessing(x)\n        x = self.TLU(x)\n        x = self.norm1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.pool1(x)\n        x = self.block5(x)\n        x = self.pool2(x)\n        x = self.block6(x)\n        x = self.pool3(x)\n        x = self.block7(x)\n        x = self.pool4(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = x.view(x.size(0), -1)\n        x = self.ip1(x)\n        return x\n\n    def reset_parameters(self):\n        for mod in self.modules():\n            if isinstance(mod, SRM_conv2d) or \\\n                    isinstance(mod, nn.BatchNorm2d) or \\\n                    isinstance(mod, ConvBlock):\n                mod.reset_parameters()\n            elif isinstance(mod, nn.Linear):\n                nn.init.normal_(mod.weight, 0. ,0.01)\n                mod.bias.data.zero_()\n\ndef accuracy(outputs, labels):\n    _, argmax = torch.max(outputs, 1)\n    return (labels == argmax.squeeze()).float().mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train function","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs, lr=2e-4, weight_decay=1e-5, device='cuda'):\n    \"\"\"Train the model with given parameters\"\"\"\n    \n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n    \n    best_val_acc = 0\n    best_model_state = None\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_losses = []\n        train_accuracies = []\n        \n        for train_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n            inputs = torch.cat((train_batch[\"cover\"], train_batch[\"stego\"]), 0)\n            labels = torch.cat((train_batch[\"label\"][0], train_batch[\"label\"][1]), 0)\n            inputs = inputs.to(device, dtype=torch.float)\n            labels = labels.to(device, dtype=torch.long)\n\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_losses.append(loss.item())\n            prediction = outputs.data.max(1)[1]\n            accuracy = (prediction.eq(labels.data).sum() * 100.0 / (labels.size()[0]))\n            train_accuracies.append(accuracy.item())\n\n        scheduler.step()  \n        train_loss_avg = np.mean(np.array(train_losses))\n        train_acc_avg = np.mean(np.array(train_accuracies))  \n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc_avg:.4f}\")\n\n        # Validation\n        model.eval()\n        val_losses = []\n        val_accuracies = []\n        \n        with torch.no_grad():\n            for val_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validating\"):\n                inputs = torch.cat((val_batch[\"cover\"], val_batch[\"stego\"]), 0)\n                labels = torch.cat((val_batch[\"label\"][0], val_batch[\"label\"][1]), 0)\n                inputs = inputs.to(device, dtype=torch.float)\n                labels = labels.to(device, dtype=torch.long)\n\n                outputs = model(inputs)\n                loss = loss_fn(outputs, labels)\n\n                val_losses.append(loss.item())\n                prediction = outputs.data.max(1)[1]\n                accuracy = (prediction.eq(labels.data).sum() * 100.0 / (labels.size()[0]))\n                val_accuracies.append(accuracy.item())\n\n            val_loss_avg = np.mean(np.array(val_losses))\n            val_acc_avg = np.mean(np.array(val_accuracies))\n            \n            print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc_avg:.4f}\")\n            \n            # Save best model\n            if val_acc_avg > best_val_acc:\n                best_val_acc = val_acc_avg\n                best_model_state = model.state_dict().copy()\n                print(f\"New best model with val acc: {best_val_acc:.4f}\")\n    \n    return best_model_state, best_val_acc\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test function","metadata":{}},{"cell_type":"code","source":"def test_model(model, test_loader, device='cuda'):\n    \"\"\"Test the model on test dataset\"\"\"\n    model.eval()\n    test_accuracies = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n            inputs = inputs.to(device, dtype=torch.float)\n            labels = labels.to(device, dtype=torch.long)\n\n            outputs = model(inputs)\n            prediction = outputs.data.max(1)[1]\n            accuracy = (prediction.eq(labels.data).sum() * 100.0 / (labels.size()[0]))\n            test_accuracies.append(accuracy.item())\n    \n    test_acc_avg = np.mean(np.array(test_accuracies))\n    print(f'Test Accuracy: {test_acc_avg:.4f}%')\n    \n    return test_acc_avg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Incremental training function","metadata":{}},{"cell_type":"code","source":"# Main incremental training function\ndef incremental_training(data_dir, model_type='SRNet', start_size=10, max_size=100, step_size=10,\n                         epochs=100, batch_size=4, val_batch_size=4, test_batch_size=16,\n                         device='cuda'):\n    \"\"\"\n    Train models incrementally with increasing dataset sizes and evaluate on test set\n    \n    Args:\n        data_dir: Directory containing train, val, and test data\n        model_type: String specifying which model to use ('SRNet', 'XuNet', or 'YeNet')\n        start_size: Initial number of cover-stego pairs to use\n        max_size: Maximum number of cover-stego pairs to use\n        step_size: Increment size for dataset expansion\n        epochs: Number of training epochs per dataset size\n        batch_size: Batch size for training\n        val_batch_size: Batch size for validation\n        test_batch_size: Batch size for testing\n        device: Device to use for training ('cuda' or 'cpu')\n    \n    Returns:\n        Dictionary of test accuracies for each dataset size\n    \"\"\"\n    train_dir = os.path.join(data_dir, 'train')\n    val_dir = os.path.join(data_dir, 'val')\n    test_dir = os.path.join(data_dir, 'test')\n    \n    # Prepare validation and test loaders\n    val_loader = get_val_loader(val_dir, val_batch_size)\n    test_loader = get_test_loader(test_dir, test_batch_size)\n    \n    # Dictionary to store results\n    test_accuracies = {}\n    \n    # Loop through different dataset sizes\n    for num_pairs in range(start_size, max_size + step_size, step_size):\n        print(f\"\\n{'='*50}\")\n        print(f\"Training {model_type} with {num_pairs} cover-stego pairs\")\n        print(f\"{'='*50}\")\n        \n        # Get training loader with specified subset size\n        train_loader = get_train_loader(train_dir, batch_size, subset_size=num_pairs)\n        \n        # Initialize the specified model for each run\n        if model_type == 'SRNet':\n            model = SRNet().to(device)\n        elif model_type == 'XuNet':\n            model = XuNet().to(device)\n        elif model_type == 'YeNet':\n            model = YeNet().to(device)\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}. Choose from 'SRNet', 'XuNet', or 'YeNet'\")\n        \n        # Train the model\n        best_model_state, best_val_acc = train_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            epochs=epochs,\n            device=device\n        )\n        \n        # Load best model for testing\n        model.load_state_dict(best_model_state)\n        \n        # Test the model\n        test_acc = test_model(model, test_loader, device)\n        \n        # Store results\n        test_accuracies[num_pairs] = test_acc\n        \n        # Print results so far\n        print(f\"\\nTest Accuracies for {model_type} so far:\")\n        for size, acc in test_accuracies.items():\n            print(f\"{size} pairs: {acc:.4f}%\")\n    \n    return test_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:23:30.480164Z","iopub.execute_input":"2025-04-06T12:23:30.480505Z","iopub.status.idle":"2025-04-06T12:23:30.488765Z","shell.execute_reply.started":"2025-04-06T12:23:30.480481Z","shell.execute_reply":"2025-04-06T12:23:30.487592Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def run_incremental_training(data_dir, model_type='SRNet', start_size=10, max_size=100, step_size=10, epochs=100):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    print(f\"Running incremental training for {model_type} model\")\n    \n    accuracies = incremental_training(\n        data_dir=data_dir,\n        model_type=model_type,\n        start_size=start_size,\n        max_size=max_size,\n        step_size=step_size,\n        epochs=epochs,\n        device=device\n    )\n    \n    print(f\"\\n=== Final Results for {model_type} ===\")\n    for size, acc in sorted(accuracies.items()):\n        print(f\"{size} pairs: {acc:.4f}%\")\n        \n    return accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:23:41.816580Z","iopub.execute_input":"2025-04-06T12:23:41.816954Z","iopub.status.idle":"2025-04-06T12:23:41.822558Z","shell.execute_reply.started":"2025-04-06T12:23:41.816923Z","shell.execute_reply":"2025-04-06T12:23:41.821632Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Example of how to run for all three models\ndef run_all_models(data_dir, start_size=10, max_size=100, step_size=10, epochs=100):\n    model_types = ['SRNet', 'XuNet', 'YeNet']\n    all_results = {}\n    \n    for model_type in model_types:\n        print(f\"\\n\\n{'#'*60}\")\n        print(f\"# Starting evaluation for {model_type}\")\n        print(f\"{'#'*60}\\n\")\n        \n        results = run_incremental_training(\n            data_dir=data_dir,\n            model_type=model_type,\n            start_size=start_size,\n            max_size=max_size,\n            step_size=step_size,\n            epochs=epochs\n        )\n        \n        all_results[model_type] = results\n    \n    return all_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_results = run_all_models()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(final_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}